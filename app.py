import os
import tempfile

# ====================== 1. æ ¸å¿ƒï¼šè‡ªåŠ¨è®¾ç½®ä¸´æ—¶è·¯å¾„ ======================
def set_custom_temp_dir():
    """è‡ªåŠ¨è®¾ç½®å›ºå®šçš„ä¸´æ—¶è·¯å¾„ï¼ˆæ— äº¤äº’è¾“å…¥ï¼‰"""
    custom_temp_path = "D:/temp1"
    try:
        if not os.path.exists(custom_temp_path):
            os.makedirs(custom_temp_path, exist_ok=True)
            print(f"å·²åˆ›å»ºè‡ªå®šä¹‰ä¸´æ—¶æ–‡ä»¶å¤¹: {custom_temp_path}")
        
        # éªŒè¯è·¯å¾„å¯å†™æ€§
        test_file = os.path.join(custom_temp_path, "test_write.txt")
        with open(test_file, "w") as f:
            f.write("test")
        os.remove(test_file)
        
        # ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼ˆä»…å½“å‰ç¨‹åºæœ‰æ•ˆï¼‰
        os.environ["TMP"] = custom_temp_path
        os.environ["TEMP"] = custom_temp_path
        
        # éªŒè¯ä¸´æ—¶è·¯å¾„ç”Ÿæ•ˆ
        temp_file = tempfile.NamedTemporaryFile(dir=custom_temp_path, delete=False)
        temp_file_path = temp_file.name
        temp_file.close()
        os.remove(temp_file_path)
        
        print(f"ä¸´æ—¶è·¯å¾„å·²è‡ªåŠ¨è®¾ç½®ä¸º: {custom_temp_path}")
        return True
    except PermissionError:
        print(f"é”™è¯¯ï¼šæ— æƒé™è®¿é—® {custom_temp_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„æƒé™")
        return False
    except Exception as e:
        print(f"è®¾ç½®ä¸´æ—¶è·¯å¾„å¤±è´¥: {str(e)}")
        return False

# å…ˆè®¾ç½®ä¸´æ—¶è·¯å¾„ï¼ˆå¿…é¡»åœ¨å¯¼å…¥scipy/sklearnç­‰åº“ä¹‹å‰æ‰§è¡Œï¼‰
set_custom_temp_dir()
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import folium
from streamlit_folium import st_folium
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import math
import io

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(layout="wide", page_title="å¤§å¹³åŸæ¤è¢«ä¸å¹²æ—±ç›‘æµ‹ä»ªè¡¨ç›˜")

# ==========================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ==========================================

@st.cache_data
def load_data():
    # è¿™é‡Œå‡è®¾æ–‡ä»¶åä¸º GreatPlains_8day_merged.csv å¹¶ä¸”åœ¨åŒä¸€ç›®å½•ä¸‹
    # å®é™…è¿è¡Œæ—¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    file_path = 'D:/12138/å¤§æ•°æ®ç³»ç»ŸåŸç†ä¸åº”ç”¨/æœŸæœ«ä½œä¸š/æ•°æ®/GreatPlains_8day_merged.csv'
    
    try:
        # å°è¯•è¯»å–æœ¬åœ°æ–‡ä»¶
        df = pd.read_csv(file_path, sep='\t') # ä½ çš„æ•°æ®çœ‹èµ·æ¥åƒæ˜¯åˆ¶è¡¨ç¬¦æˆ–ç©ºæ ¼åˆ†éš”
        if df.shape[1] == 1: # å¦‚æœåˆ†éš”ç¬¦ä¸å¯¹ï¼Œå°è¯•é€—å·
             df = pd.read_csv(file_path, sep=',')
    except FileNotFoundError:
        st.error(f"æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}ã€‚è¯·ç¡®ä¿CSVæ–‡ä»¶åœ¨è„šæœ¬è¿è¡Œç›®å½•ä¸‹ã€‚")
        return None

    # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date')
    
    # åˆ—åé‡å‘½åï¼ˆä¸ºäº†æ˜¾ç¤ºæ›´å‹å¥½ï¼‰
    df = df.rename(columns={
        'ndvi': 'NDVI (æ¤è¢«æŒ‡æ•°)',
        'precip_8d_sum_mm': '8æ—¥ç´¯è®¡é™æ°´ (mm)',
        'soil_moisture_8d_mean': '8æ—¥å¹³å‡åœŸå£¤æ¹¿åº¦ (mÂ³/mÂ³)',
        'temp_8d_mean_C': '8æ—¥å¹³å‡æ°”æ¸© (Â°C)',
        'pet_8d_sum_mm': '8æ—¥ç´¯è®¡æ½œåœ¨è’¸æ•£ (mm)'
    })
    
    return df

# ==========================================
# 2. æ¨¡å‹å®šä¹‰ (Transformer for Time Series)
# ==========================================

class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim, dropout=0.1):
        super(TimeSeriesTransformer, self).__init__()
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.decoder = nn.Linear(d_model, output_dim)
        self.embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)

    def forward(self, src):
        # src shape: [batch_size, seq_len, input_dim]
        src = self.embedding(src) # [batch, seq, d_model]
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = output[:, -1, :] # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        output = self.decoder(output)
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# æ•°æ®å‡†å¤‡å‡½æ•°
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length] # é¢„æµ‹ä¸‹ä¸€æ­¥
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# ==========================================
# 3. é¡µé¢å¸ƒå±€ä¸é€»è¾‘
# ==========================================

def main():
    st.title("ğŸŒ¿ ç¾å›½å—éƒ¨å¤§å¹³åŸæ¤è¢«å¹²æ—±ç›‘æµ‹ä¸å¤šæºé©±åŠ¨é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("""
    > **æ•°æ®æ¥æºè¯´æ˜**ï¼š
    > æœ¬ç³»ç»ŸåŸºäº **MODIS (NDVI)**, **CHIRPS (é™æ°´)**, **ERA5-Land (åœŸå£¤æ¹¿åº¦ã€æ°”æ¸©ã€è’¸æ•£)** æ„å»ºäº† 2000-2024 å¹´çš„å¤šæºæ—¶é—´åºåˆ—æ•°æ®é›†ã€‚
    > æ•°æ®å·²ç»Ÿä¸€å¤„ç†ä¸º 8 å¤©æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¹¶é’ˆå¯¹ç ”ç©¶åŒºåŸŸï¼ˆ105Â°Wâ€“95Â°Wï¼Œ32Â°Nâ€“40Â°Nï¼‰è¿›è¡Œäº†åŒºåŸŸèšåˆã€‚
    """)

    df = load_data()
    if df is None:
        return

    # ä¾§è¾¹æ æ§åˆ¶
    st.sidebar.header("å…¨å±€è®¾ç½®")
    
    # ç‰¹å¾é€‰æ‹©
    feature_cols = ['NDVI (æ¤è¢«æŒ‡æ•°)', '8æ—¥ç´¯è®¡é™æ°´ (mm)', '8æ—¥å¹³å‡åœŸå£¤æ¹¿åº¦ (mÂ³/mÂ³)', '8æ—¥å¹³å‡æ°”æ¸© (Â°C)', '8æ—¥ç´¯è®¡æ½œåœ¨è’¸æ•£ (mm)']
    selected_feature = st.sidebar.selectbox("é€‰æ‹©ä¸»è¦è§‚æµ‹ç‰¹å¾", feature_cols, index=0)
    
    # TABé¡µåˆ‡æ¢
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š æ•°æ®æ¦‚è§ˆä¸ç»Ÿè®¡", "ğŸ—ºï¸ åŒºåŸŸåŠ¨æ€åœ°å›¾", "ğŸ¤– Transformer æœªæ¥é¢„æµ‹"])

    # --- TAB 1: æ•°æ®æ¦‚è§ˆ ---
    with tab1:
        st.header("å†å²æ•°æ®æ—¶é—´åºåˆ—åˆ†æ")
        
        # ç»˜åˆ¶ä¸»ç‰¹å¾æ›²çº¿
        fig_main = px.line(df, x='date', y=selected_feature, title=f"{selected_feature} 2000-2024 å˜åŒ–è¶‹åŠ¿")
        fig_main.update_layout(hovermode="x unified")
        st.plotly_chart(fig_main, use_container_width=True)
        
        col1, col2 = st.columns([2, 1])
        with col1:
            # å¤šå˜é‡å¯¹æ¯”å›¾
            st.subheader("å¤šå˜é‡ååŒå˜åŒ–")
            compare_features = st.multiselect("é€‰æ‹©è¦å¯¹æ¯”çš„ç‰¹å¾", feature_cols, default=['NDVI (æ¤è¢«æŒ‡æ•°)', '8æ—¥ç´¯è®¡é™æ°´ (mm)'])
            if compare_features:
                # æ ‡å‡†åŒ–ä»¥ä¾¿åœ¨åŒä¸€è½´ä¸Šæ˜¾ç¤ºè¶‹åŠ¿
                df_norm = df.copy()
                for col in compare_features:
                    df_norm[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())
                
                fig_compare = px.line(df_norm, x='date', y=compare_features, title="æ ‡å‡†åŒ–è¶‹åŠ¿å¯¹æ¯” (å½’ä¸€åŒ– 0-1)")
                st.plotly_chart(fig_compare, use_container_width=True)
        
        with col2:
            # ç›¸å…³æ€§çƒ­åŠ›å›¾
            st.subheader("ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
            corr = df[feature_cols].corr()
            fig_corr = px.imshow(corr, text_auto=True, color_continuous_scale='RdBu_r', title="Pearson ç›¸å…³ç³»æ•°")
            st.plotly_chart(fig_corr, use_container_width=True)
            
            st.info("æç¤ºï¼šNDVI é€šå¸¸ä¸é™æ°´å’ŒåœŸå£¤æ¹¿åº¦å‘ˆæ­£ç›¸å…³ï¼Œä¸æ½œåœ¨è’¸æ•£å¯èƒ½å‘ˆå¤æ‚å…³ç³»ï¼ˆè§†æ°´åˆ†é™åˆ¶æ¡ä»¶è€Œå®šï¼‰ã€‚")

    # --- TAB 2: åœ°å›¾å¯è§†åŒ– ---
    with tab2:
        st.header("ç ”ç©¶åŒºåŸŸåŠ¨æ€ç›‘æµ‹")
        st.markdown("é€šè¿‡æ‹–åŠ¨ä¸‹æ–¹è¿›åº¦æ¡ï¼ŒæŸ¥çœ‹é€‰å®šç‰¹å¾åœ¨**å¤§å¹³åŸç ”ç©¶åŒº (105Â°Wâ€“95Â°W, 32Â°Nâ€“40Â°N)** å†…éšæ—¶é—´çš„å˜åŒ–æƒ…å†µã€‚")
        st.markdown("*æ³¨ï¼šç”±äºæ•°æ®å·²è¿›è¡ŒåŒºåŸŸå¹³å‡å¤„ç†ï¼Œåœ°å›¾é¢œè‰²ä»£è¡¨æ•´ä¸ªåŒºåŸŸåœ¨è¯¥æ—¶é—´ç‚¹çš„å¹³å‡çŠ¶æ€ã€‚*")

        # æ—¶é—´æ»‘å—
        min_date = df['date'].min().to_pydatetime()
        max_date = df['date'].max().to_pydatetime()
        
        selected_date = st.slider(
            "é€‰æ‹©æ—¶é—´",
            min_value=min_date,
            max_value=max_date,
            value=min_date,
            format="YYYY-MM-DD"
        )
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ•°æ®
        nearest_row = df.iloc[(df['date'] - selected_date).abs().argsort()[:1]]
        current_val = nearest_row[selected_feature].values[0]
        current_date_str = nearest_row['date'].dt.strftime('%Y-%m-%d').values[0]

        # è®¡ç®—é¢œè‰²çš„å½’ä¸€åŒ–å€¼
        min_val = df[selected_feature].min()
        max_val = df[selected_feature].max()
        
        # å®šä¹‰é¢œè‰²æ˜ å°„é€»è¾‘
        import matplotlib.cm as cm
        import matplotlib.colors as mcolors

        norm = mcolors.Normalize(vmin=min_val, vmax=max_val)
        # æ ¹æ®ç‰¹å¾é€‰æ‹©ä¸åŒçš„è‰²å¸¦
        if 'NDVI' in selected_feature:
            cmap = cm.get_cmap('RdYlGn') # çº¢-é»„-ç»¿ (æ¤è¢«)
        elif 'é™æ°´' in selected_feature or 'åœŸå£¤' in selected_feature:
            cmap = cm.get_cmap('Blues') # è“ (æ°´)
        elif 'æ°”æ¸©' in selected_feature or 'è’¸æ•£' in selected_feature:
            cmap = cm.get_cmap('YlOrRd') # é»„-çº¢ (çƒ­)
        else:
            cmap = cm.get_cmap('viridis')

        rgba = cmap(norm(current_val))
        hex_color = mcolors.to_hex(rgba)

        # åˆ›å»ºåœ°å›¾
        m = folium.Map(location=[36, -100], zoom_start=6, tiles="CartoDB positron")

        # ç»˜åˆ¶çŸ©å½¢åŒºåŸŸ
        bounds = [[32, -105], [40, -95]] # Lat range, Lon range
        
        folium.Rectangle(
            bounds=bounds,
            color=hex_color,
            fill=True,
            fill_color=hex_color,
            fill_opacity=0.7,
            popup=folium.Popup(f"<b>æ—¥æœŸ:</b> {current_date_str}<br><b>{selected_feature}:</b> {current_val:.4f}", max_width=300),
            tooltip="ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…"
        ).add_to(m)

        # æ˜¾ç¤ºåœ°å›¾
        col_map, col_info = st.columns([3, 1])
        with col_map:
            st_folium(m, width=800, height=500)
        
        with col_info:
            st.metric(label=f"å½“å‰æ—¥æœŸ: {current_date_str}", value=f"{current_val:.4f}")
            st.write(f"**{selected_feature}** åœ¨å…¨æ—¶æ®µå†…çš„èŒƒå›´:")
            st.write(f"æœ€å°å€¼: {min_val:.4f}")
            st.write(f"æœ€å¤§å€¼: {max_val:.4f}")
            st.progress((current_val - min_val) / (max_val - min_val))

    # --- TAB 3: é¢„æµ‹å»ºæ¨¡ ---
    with tab3:
        st.header("å¤šæºé©±åŠ¨æœªæ¥é¢„æµ‹ (Transformer)")
        st.markdown("""
        æœ¬æ¨¡å—åˆ©ç”¨ **Transformer** æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå­¦ä¹  NDVI ä¸æ°”è±¡å› å­ï¼ˆé™æ°´ã€æ°”æ¸©ç­‰ï¼‰çš„å†å²æ—¶åºå…³ç³»ã€‚
        æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå°†æ‰§è¡Œ**å¤šæ­¥æ»šåŠ¨é¢„æµ‹**ï¼Œç”Ÿæˆè¶…å‡ºå½“å‰æ•°æ®é›†æ—¶é—´èŒƒå›´çš„æœªæ¥è¶‹åŠ¿ã€‚
        """)

        col_params, col_train = st.columns([1, 3])
        
        with col_params:
            st.subheader("æ¨¡å‹äº¤äº’å‚æ•°")
            forecast_steps = st.number_input("æœªæ¥é¢„æµ‹æ­¥æ•° (æ¯æ­¥8å¤©)", min_value=4, max_value=46, value=12, help="é¢„æµ‹æœªæ¥å¤šå°‘ä¸ª8å¤©å‘¨æœŸ")
            seq_length = st.slider("å›é¡¾çª—å£å¤§å° (Seq Length)", 4, 24, 12, help="æ¨¡å‹åˆ©ç”¨è¿‡å»å¤šå°‘ä¸ªæ—¶é—´æ­¥æ¥é¢„æµ‹ä¸‹ä¸€æ­¥")
            epochs = st.slider("è®­ç»ƒè½®æ¬¡ (Epochs)", 10, 200, 50)
            hidden_dim = st.selectbox("Transformer éšè—å±‚ç»´åº¦", [32, 64, 128], index=1)
            
            start_train = st.button("ğŸš€ å¼€å§‹è®­ç»ƒå¹¶é¢„æµ‹", type="primary")

        if start_train:
            with col_train:
                status_text = st.empty()
                progress_bar = st.progress(0)
                
                # 1. æ•°æ®å‡†å¤‡
                status_text.text("æ­£åœ¨å‡†å¤‡å¼ é‡æ•°æ®...")
                
                # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾è¿›è¡Œé¢„æµ‹
                data_values = df[feature_cols].values
                scaler = MinMaxScaler()
                data_scaled = scaler.fit_transform(data_values)
                
                # ç›®æ ‡æ˜¯é¢„æµ‹ NDVI (feature_cols[0])
                # è¿™é‡Œçš„ç®€å•æ¼”ç¤ºæ˜¯ï¼šç”¨è¿‡å»Nå¤©çš„æ‰€æœ‰ç‰¹å¾ï¼Œé¢„æµ‹ä¸‹ä¸€å¤©çš„æ‰€æœ‰ç‰¹å¾ï¼ˆæˆ–è€…ä»…NDVIï¼‰
                # ä¸ºäº†æ”¯æŒå¤šæ­¥æ»šåŠ¨é¢„æµ‹ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹é¢„æµ‹æ‰€æœ‰ç‰¹å¾ï¼Œä»¥ä¾¿å°†é¢„æµ‹å€¼ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
                
                X, y = create_sequences(data_scaled, seq_length)
                
                # è½¬æ¢ä¸º Tensor
                X_tensor = torch.FloatTensor(X)
                y_tensor = torch.FloatTensor(y) # é¢„æµ‹æ‰€æœ‰ç‰¹å¾
                
                # åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›† (è¿™é‡Œä¸»è¦ç”¨å…¨é‡æ•°æ®è®­ç»ƒä»¥è·å¾—æœ€å¥½çš„æœªæ¥é¢„æµ‹èƒ½åŠ›ï¼Œæˆ–è€…ç•™ä¸€å°éƒ¨åˆ†éªŒè¯)
                train_size = int(len(X) * 0.9)
                X_train, X_val = X_tensor[:train_size], X_tensor[train_size:]
                y_train, y_val = y_tensor[:train_size], y_tensor[train_size:]
                
                # 2. æ¨¡å‹åˆå§‹åŒ–
                input_dim = len(feature_cols)
                output_dim = len(feature_cols) # è¾“å‡ºæ‰€æœ‰ç‰¹å¾ä»¥æ”¯æŒæ»šåŠ¨é¢„æµ‹
                
                model = TimeSeriesTransformer(
                    input_dim=input_dim, 
                    d_model=hidden_dim, 
                    nhead=4, 
                    num_layers=2, 
                    output_dim=output_dim
                )
                
                criterion = nn.MSELoss()
                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
                
                # 3. è®­ç»ƒå¾ªç¯
                model.train()
                train_losses = []
                
                for epoch in range(epochs):
                    optimizer.zero_grad()
                    output = model(X_train)
                    loss = criterion(output, y_train)
                    loss.backward()
                    optimizer.step()
                    
                    train_losses.append(loss.item())
                    
                    if (epoch + 1) % 10 == 0:
                        progress_bar.progress((epoch + 1) / epochs)
                        status_text.text(f"è®­ç»ƒä¸­... Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}")

                status_text.text("è®­ç»ƒå®Œæˆï¼æ­£åœ¨è¿›è¡Œæœªæ¥æ¨æ¼”...")
                
                # 4. æœªæ¥å¤šæ­¥é¢„æµ‹ (Extrapolation)
                model.eval()
                future_predictions = []
                
                # åˆå§‹è¾“å…¥ï¼šæ•°æ®çš„æœ€å seq_length ä¸ªæ—¶é—´æ­¥
                current_seq = torch.FloatTensor(data_scaled[-seq_length:]).unsqueeze(0) # [1, seq_len, input_dim]
                
                with torch.no_grad():
                    for _ in range(forecast_steps):
                        # é¢„æµ‹ä¸‹ä¸€æ­¥
                        next_step = model(current_seq) # [1, output_dim]
                        
                        # ä¿å­˜é¢„æµ‹ç»“æœ
                        future_predictions.append(next_step.numpy()[0])
                        
                        # æ›´æ–°è¾“å…¥åºåˆ—ï¼šç§»é™¤ç¬¬ä¸€ä¸ªï¼ŒåŠ å…¥é¢„æµ‹å‡ºçš„è¿™ä¸€ä¸ª
                        # next_step shape is [1, feature_dim], need to reshape to match dim
                        next_step_reshaped = next_step.unsqueeze(1) # [1, 1, input_dim]
                        current_seq = torch.cat((current_seq[:, 1:, :], next_step_reshaped), dim=1)

                # 5. åå½’ä¸€åŒ–ä¸å¯è§†åŒ–
                future_pred_scaled = np.array(future_predictions)
                future_pred_original = scaler.inverse_transform(future_pred_scaled)
                
                # æ„å»ºæœªæ¥æ—¶é—´è½´
                last_date = df['date'].iloc[-1]
                future_dates = [last_date + pd.Timedelta(days=8 * (i + 1)) for i in range(forecast_steps)]
                
                # åˆ›å»ºé¢„æµ‹ç»“æœ DataFrame
                pred_df = pd.DataFrame(future_pred_original, columns=feature_cols)
                pred_df['date'] = future_dates
                pred_df['type'] = 'æœªæ¥é¢„æµ‹ (Forecast)'
                
                # å†å²æ•°æ®æœ€åä¸€æ®µç”¨äºè¿æ¥
                history_plot_df = df.tail(100).copy() # åªç”»æœ€è¿‘100ä¸ªç‚¹é¿å…å›¾å¤ªæŒ¤
                history_plot_df['type'] = 'å†å²è§‚æµ‹ (History)'
                
                # åˆå¹¶æ•°æ®ç”¨äºç»˜å›¾
                combined_df = pd.concat([history_plot_df, pred_df], ignore_index=True)
                
                # ç»˜åˆ¶é¢„æµ‹å›¾
                st.subheader(f"æœªæ¥ {forecast_steps*8} å¤© {selected_feature} è¶‹åŠ¿é¢„æµ‹")
                
                fig_forecast = px.line(
                    combined_df, 
                    x='date', 
                    y=selected_feature, 
                    color='type',
                    color_discrete_map={'å†å²è§‚æµ‹ (History)': 'gray', 'æœªæ¥é¢„æµ‹ (Forecast)': 'red'},
                    title=f"Transformer å¤šæ­¥æ»šåŠ¨é¢„æµ‹ç»“æœ: {selected_feature}"
                )
                # æ·»åŠ é¢„æµ‹åŒºé—´çš„èƒŒæ™¯è‰²
                fig_forecast.add_vrect(
                    x0=last_date, 
                    x1=future_dates[-1], 
                    fillcolor="red", 
                    opacity=0.1, 
                    layer="below", 
                    line_width=0,
                    annotation_text="é¢„æµ‹åŒºé—´"
                )
                
                st.plotly_chart(fig_forecast, use_container_width=True)
                
                # è®­ç»ƒæŸå¤±æ›²çº¿
                with st.expander("æŸ¥çœ‹æ¨¡å‹è®­ç»ƒæŸå¤± (Loss Curve)"):
                    st.line_chart(train_losses)
                    st.caption("MSE Loss éš Epochs ä¸‹é™æƒ…å†µï¼Œè‹¥æ›²çº¿æœªæ”¶æ•›ï¼Œè¯·å¢åŠ è®­ç»ƒè½®æ¬¡ã€‚")

if __name__ == "__main__":
    main()
